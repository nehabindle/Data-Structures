{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yelp_Neha_TensorFlow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehabindle/Data-Structures/blob/master/Yelp_Neha_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pifiySPxxOyO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "466fe431-addb-4190-b819-c4180ad7182f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s gdrive/'Team Drives'/'Data Mining Team'/ gdata"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "luXf_OC4xj-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "de3acef6-0a9c-4f54-c4da-a98be8bbacc9"
      },
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/67/f7/1462c6d28ec27ef2812aa2e9376c7fc7b39a23f0e02297f71119d74375c5/contractions-0.0.18-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_gqIeCczxrMV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tf.keras.models import Sequential  # This does not work!\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L6B4Xnbrxllw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdZuCaRCx-DJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing Libraries from Keras.\n"
      ]
    },
    {
      "metadata": {
        "id": "sV3EeIDyxyKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "loxaQYLfyFod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f33ea2e-5ad6-49ca-a7f3-57ae0bac7954"
      },
      "cell_type": "code",
      "source": [
        "#Checking the version on Tensorflow\n",
        "tf.__version__\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "aMH6BXckyNPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "740dd0c1-12b5-4095-d640-f29eb665434c"
      },
      "cell_type": "code",
      "source": [
        "#Checking the version of Keras\n",
        "tf.keras.__version__\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "lWI92Z8HyZcS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load Data\n"
      ]
    },
    {
      "metadata": {
        "id": "Pa9qrdmMyeu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "connection = sqlite3.connect('gdata/yelpHotelData.db')\n",
        "x1 = connection.execute(\"select * FROM review\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4eFQJ-AyoSS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "description = []\n",
        "Class = []\n",
        "\n",
        "data = x1.fetchmany(10000)\n",
        "\n",
        "for x in data:\n",
        "  description.append(x[3])\n",
        "  Class.append(x[8])\n",
        "trainData = {'Class' : Class, 'Description' : description}\n",
        "df_X = pd.DataFrame(trainData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXRxY9WYyr0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6a45a621-54c9-46df-e38e-0cefecefe6c9"
      },
      "cell_type": "code",
      "source": [
        "df_X['Class'].value_counts()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N     5078\n",
              "NR    2461\n",
              "YR    1681\n",
              "Y      780\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "0wxZw1G_yvwx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azw9ygo4zI5t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_X['Description'],df_X['Class'], test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WEeoIK6zzQqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6763ff0-f648-4ac8-d7cf-a94e6ba8c788"
      },
      "cell_type": "code",
      "source": [
        "#Checking the shape of Train and Test Data\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000,) (3000,)\n",
            "(7000,) (3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KHoTHysvznFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc03c458-d5b2-4ba2-bf63-7595dbd151da"
      },
      "cell_type": "code",
      "source": [
        "#Checking the length of train and test data\n",
        "print(\"Train-set size: \", len(X_train))\n",
        "print(\"Test-set size:  \", len(X_test))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train-set size:  7000\n",
            "Test-set size:   3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8mk2YOpd0bnT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Combine into one data-set for some uses below.\n",
        "data_text = X_train + X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B9HMVqX805Vo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "df0d8cee-3e63-4d86-835d-04855ac54ef1"
      },
      "cell_type": "code",
      "source": [
        "#Printing an example to see how data looks like\n",
        "X_train[1]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The only place inside the Loop that you can stay for $55/night. Also, the only place you can have a picnic dinner and get a little frisky on the 17th floor roof and then wake up in your room the next morning to an army of ants going in on your picnic leftovers.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "cyJZjUU-1GZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56b004b0-14b2-4a23-a5a7-71884cc86072"
      },
      "cell_type": "code",
      "source": [
        "y_train[1]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'N'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "GxZAyxGu1Nkq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenizer\n",
        "A neural network cannot work directly on text-strings so we must convert it somehow. There are two steps in this conversion, the first step is called the \"tokenizer\" which converts words to integers and is done on the data-set before it is input to the neural network. The second step is an integrated part of the neural network itself and is called the \"embedding\"-layer, which is described further below.\n",
        "\n",
        "We may instruct the tokenizer to only use e.g. the 10000 most popular words from the data-set.\n"
      ]
    },
    {
      "metadata": {
        "id": "sLMKCouK1VcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = 10000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gQU_A1N01b96",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The tokenizer can then be \"fitted\" to the data-set. This scans through all the text and strips it from unwanted characters such as punctuation, and also converts it to lower-case characters. The tokenizer then builds a vocabulary of all unique words along with various data-structures for accessing the data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eYGSiSEb1ebL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4886f624-0fcb-4532-cc13-027457879a98"
      },
      "cell_type": "code",
      "source": [
        "#Tokenization of train data\n",
        "%%time\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.09 s, sys: 8.28 ms, total: 1.1 s\n",
            "Wall time: 1.11 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HlNIXf2a1z0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "171a12f4-0098-4504-d54e-7a63eae48302"
      },
      "cell_type": "code",
      "source": [
        "#Tokenization of test data\n",
        "%%time\n",
        "tokenizer.fit_on_texts(X_test)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 476 ms, sys: 2.68 ms, total: 478 ms\n",
            "Wall time: 482 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lrkQiKCt185i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#set num_words=None above, and then it will automatically be set to the vocabulary-size here.\n",
        "\n",
        "\n",
        "if num_words is None:\n",
        "    num_words = len(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5cR1ZEK2JhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then inspect the vocabulary that has been gathered by the tokenizer. This is ordered by the number of occurrences of the words in the data-set. These integer-numbers are called word indices or \"tokens\" because they uniquely identify each word in the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "h89DHq5K2GsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        },
        "outputId": "78ac6e67-7f26-4557-e35e-a99753123afc"
      },
      "cell_type": "code",
      "source": [
        "tokenizer.word_index\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'to': 4,\n",
              " 'i': 5,\n",
              " 'was': 6,\n",
              " 'of': 7,\n",
              " 'in': 8,\n",
              " 'for': 9,\n",
              " 'is': 10,\n",
              " 'it': 11,\n",
              " 'that': 12,\n",
              " 'but': 13,\n",
              " 'my': 14,\n",
              " 'with': 15,\n",
              " 'on': 16,\n",
              " 'you': 17,\n",
              " 'this': 18,\n",
              " 'we': 19,\n",
              " 'hotel': 20,\n",
              " 'room': 21,\n",
              " 'at': 22,\n",
              " 'have': 23,\n",
              " 'they': 24,\n",
              " 'not': 25,\n",
              " 'were': 26,\n",
              " 'had': 27,\n",
              " 'are': 28,\n",
              " 'so': 29,\n",
              " 'there': 30,\n",
              " 'be': 31,\n",
              " 'as': 32,\n",
              " 'very': 33,\n",
              " 'here': 34,\n",
              " 'great': 35,\n",
              " 'if': 36,\n",
              " 'out': 37,\n",
              " 'from': 38,\n",
              " 'all': 39,\n",
              " 'place': 40,\n",
              " 'me': 41,\n",
              " 'our': 42,\n",
              " 'like': 43,\n",
              " 'good': 44,\n",
              " 'one': 45,\n",
              " 'or': 46,\n",
              " 'up': 47,\n",
              " 'get': 48,\n",
              " 'just': 49,\n",
              " '\\xa0the': 50,\n",
              " 'nice': 51,\n",
              " 'would': 52,\n",
              " 'when': 53,\n",
              " 'an': 54,\n",
              " '\\xa0': 55,\n",
              " \"it's\": 56,\n",
              " 'about': 57,\n",
              " 'which': 58,\n",
              " 'service': 59,\n",
              " 'stay': 60,\n",
              " 'really': 61,\n",
              " '\\xa0i': 62,\n",
              " 'time': 63,\n",
              " 'no': 64,\n",
              " 'some': 65,\n",
              " 'night': 66,\n",
              " 'rooms': 67,\n",
              " 'their': 68,\n",
              " 'can': 69,\n",
              " 'location': 70,\n",
              " 'food': 71,\n",
              " 'only': 72,\n",
              " 'more': 73,\n",
              " 'staff': 74,\n",
              " 'your': 75,\n",
              " 'also': 76,\n",
              " 'chicago': 77,\n",
              " 'back': 78,\n",
              " 'got': 79,\n",
              " 'by': 80,\n",
              " 'us': 81,\n",
              " 'go': 82,\n",
              " 'other': 83,\n",
              " 'what': 84,\n",
              " 'been': 85,\n",
              " 'because': 86,\n",
              " 'bar': 87,\n",
              " 'little': 88,\n",
              " 'than': 89,\n",
              " 'could': 90,\n",
              " 'even': 91,\n",
              " \"didn't\": 92,\n",
              " \"don't\": 93,\n",
              " 'well': 94,\n",
              " 'pretty': 95,\n",
              " 'has': 96,\n",
              " \"i'm\": 97,\n",
              " 'too': 98,\n",
              " 'will': 99,\n",
              " 'do': 100,\n",
              " 'stayed': 101,\n",
              " 'did': 102,\n",
              " 'people': 103,\n",
              " 'after': 104,\n",
              " 'day': 105,\n",
              " 'small': 106,\n",
              " 'much': 107,\n",
              " 'them': 108,\n",
              " 'again': 109,\n",
              " 'two': 110,\n",
              " 'free': 111,\n",
              " 'friendly': 112,\n",
              " 'right': 113,\n",
              " '2': 114,\n",
              " 'down': 115,\n",
              " 'bed': 116,\n",
              " 'check': 117,\n",
              " 'lobby': 118,\n",
              " 'floor': 119,\n",
              " 'first': 120,\n",
              " 'clean': 121,\n",
              " 'he': 122,\n",
              " 'over': 123,\n",
              " 'around': 124,\n",
              " 'area': 125,\n",
              " 'off': 126,\n",
              " \"i've\": 127,\n",
              " 'next': 128,\n",
              " 'desk': 129,\n",
              " 'who': 130,\n",
              " 'front': 131,\n",
              " 'bit': 132,\n",
              " 'bathroom': 133,\n",
              " 'she': 134,\n",
              " 'way': 135,\n",
              " 'know': 136,\n",
              " 'few': 137,\n",
              " 'think': 138,\n",
              " 'any': 139,\n",
              " 'came': 140,\n",
              " 'best': 141,\n",
              " 'better': 142,\n",
              " 'though': 143,\n",
              " 'made': 144,\n",
              " 'definitely': 145,\n",
              " 'want': 146,\n",
              " 'never': 147,\n",
              " 'view': 148,\n",
              " 'into': 149,\n",
              " 'restaurant': 150,\n",
              " 'price': 151,\n",
              " 'love': 152,\n",
              " 'make': 153,\n",
              " '3': 154,\n",
              " 'going': 155,\n",
              " 'then': 156,\n",
              " \"you're\": 157,\n",
              " '5': 158,\n",
              " 'went': 159,\n",
              " 'everything': 160,\n",
              " 'always': 161,\n",
              " 'while': 162,\n",
              " 'breakfast': 163,\n",
              " 'city': 164,\n",
              " 'how': 165,\n",
              " 'before': 166,\n",
              " 'see': 167,\n",
              " 'hotels': 168,\n",
              " 'most': 169,\n",
              " 'where': 170,\n",
              " 'come': 171,\n",
              " 'find': 172,\n",
              " 'experience': 173,\n",
              " 'many': 174,\n",
              " 'am': 175,\n",
              " 'still': 176,\n",
              " 'comfortable': 177,\n",
              " 'said': 178,\n",
              " 'sure': 179,\n",
              " 'big': 180,\n",
              " 'take': 181,\n",
              " 'door': 182,\n",
              " '\\xa0we': 183,\n",
              " 'say': 184,\n",
              " 'new': 185,\n",
              " 'since': 186,\n",
              " 'old': 187,\n",
              " 'being': 188,\n",
              " 'parking': 189,\n",
              " 'every': 190,\n",
              " 'her': 191,\n",
              " 'away': 192,\n",
              " '4': 193,\n",
              " \"wasn't\": 194,\n",
              " 'work': 195,\n",
              " 'bad': 196,\n",
              " '\\xa0it': 197,\n",
              " 'walk': 198,\n",
              " \"can't\": 199,\n",
              " 'its': 200,\n",
              " 'minutes': 201,\n",
              " 'ever': 202,\n",
              " 'another': 203,\n",
              " 'water': 204,\n",
              " 'last': 205,\n",
              " 'looking': 206,\n",
              " 'something': 207,\n",
              " '1': 208,\n",
              " 'need': 209,\n",
              " 'lot': 210,\n",
              " 'feel': 211,\n",
              " 'things': 212,\n",
              " 'street': 213,\n",
              " 'enough': 214,\n",
              " 'thing': 215,\n",
              " 'shower': 216,\n",
              " 'coffee': 217,\n",
              " 'took': 218,\n",
              " 'downtown': 219,\n",
              " 'close': 220,\n",
              " 'try': 221,\n",
              " 'amazing': 222,\n",
              " 'times': 223,\n",
              " 'give': 224,\n",
              " 'helpful': 225,\n",
              " 'wedding': 226,\n",
              " 'ordered': 227,\n",
              " 'quite': 228,\n",
              " 'menu': 229,\n",
              " 'business': 230,\n",
              " \"i'd\": 231,\n",
              " 'park': 232,\n",
              " 'stars': 233,\n",
              " 'now': 234,\n",
              " 'through': 235,\n",
              " 'large': 236,\n",
              " 'dinner': 237,\n",
              " 'side': 238,\n",
              " 'told': 239,\n",
              " 'beautiful': 240,\n",
              " 'long': 241,\n",
              " 'awesome': 242,\n",
              " 'order': 243,\n",
              " 'overall': 244,\n",
              " 'actually': 245,\n",
              " 'weekend': 246,\n",
              " 'decent': 247,\n",
              " 'michigan': 248,\n",
              " 'use': 249,\n",
              " 'should': 250,\n",
              " 'drinks': 251,\n",
              " 'asked': 252,\n",
              " 'beds': 253,\n",
              " '10': 254,\n",
              " 'wait': 255,\n",
              " 'staying': 256,\n",
              " 'huge': 257,\n",
              " '\\xa0they': 258,\n",
              " 'recommend': 259,\n",
              " 'nothing': 260,\n",
              " 'star': 261,\n",
              " 'look': 262,\n",
              " 'hot': 263,\n",
              " \"that's\": 264,\n",
              " 'wanted': 265,\n",
              " 'found': 266,\n",
              " 'walking': 267,\n",
              " 'super': 268,\n",
              " 'morning': 269,\n",
              " 'top': 270,\n",
              " 'line': 271,\n",
              " 'called': 272,\n",
              " 'perfect': 273,\n",
              " 'chicken': 274,\n",
              " 'deal': 275,\n",
              " 'hour': 276,\n",
              " 'loved': 277,\n",
              " 'decor': 278,\n",
              " 'during': 279,\n",
              " 'happy': 280,\n",
              " 'cool': 281,\n",
              " 'same': 282,\n",
              " 'fresh': 283,\n",
              " 'restaurants': 284,\n",
              " 'however': 285,\n",
              " 'pay': 286,\n",
              " 'home': 287,\n",
              " 'open': 288,\n",
              " 'full': 289,\n",
              " 'thought': 290,\n",
              " 'kind': 291,\n",
              " 'used': 292,\n",
              " 'both': 293,\n",
              " 'w': 294,\n",
              " 'internet': 295,\n",
              " 'outside': 296,\n",
              " 'probably': 297,\n",
              " 'worth': 298,\n",
              " 'town': 299,\n",
              " 'modern': 300,\n",
              " 'couple': 301,\n",
              " 'having': 302,\n",
              " 'excellent': 303,\n",
              " 'tv': 304,\n",
              " 'anything': 305,\n",
              " 'across': 306,\n",
              " 'table': 307,\n",
              " 'put': 308,\n",
              " 'getting': 309,\n",
              " 'places': 310,\n",
              " 'although': 311,\n",
              " 'friends': 312,\n",
              " 'drink': 313,\n",
              " 'visit': 314,\n",
              " 'those': 315,\n",
              " 'suite': 316,\n",
              " 'delicious': 317,\n",
              " 'these': 318,\n",
              " 'why': 319,\n",
              " 'wine': 320,\n",
              " 'ok': 321,\n",
              " 'each': 322,\n",
              " 'different': 323,\n",
              " 'eat': 324,\n",
              " 'left': 325,\n",
              " 'review': 326,\n",
              " 'without': 327,\n",
              " 'size': 328,\n",
              " 'building': 329,\n",
              " 'etc': 330,\n",
              " 'everyone': 331,\n",
              " 'space': 332,\n",
              " 'pool': 333,\n",
              " 'car': 334,\n",
              " 'ask': 335,\n",
              " 'call': 336,\n",
              " 'friend': 337,\n",
              " 'able': 338,\n",
              " 'high': 339,\n",
              " 'cheese': 340,\n",
              " 'his': 341,\n",
              " 'days': 342,\n",
              " 'house': 343,\n",
              " 'hard': 344,\n",
              " 'needed': 345,\n",
              " 'special': 346,\n",
              " 'center': 347,\n",
              " 'looked': 348,\n",
              " 'late': 349,\n",
              " 'enjoyed': 350,\n",
              " 'hours': 351,\n",
              " 'guests': 352,\n",
              " 'let': 353,\n",
              " 'else': 354,\n",
              " 'especially': 355,\n",
              " 'fine': 356,\n",
              " 'gave': 357,\n",
              " 'once': 358,\n",
              " 'someone': 359,\n",
              " 'lunch': 360,\n",
              " '30': 361,\n",
              " 'part': 362,\n",
              " 'seemed': 363,\n",
              " 'prices': 364,\n",
              " \"there's\": 365,\n",
              " 'elevator': 366,\n",
              " 'far': 367,\n",
              " 'maybe': 368,\n",
              " 'felt': 369,\n",
              " 'end': 370,\n",
              " 'money': 371,\n",
              " 'meal': 372,\n",
              " 'party': 373,\n",
              " 'shopping': 374,\n",
              " 'booked': 375,\n",
              " 'selection': 376,\n",
              " 'until': 377,\n",
              " 'located': 378,\n",
              " 'extra': 379,\n",
              " 'nights': 380,\n",
              " 'person': 381,\n",
              " 'least': 382,\n",
              " 'wonderful': 383,\n",
              " 'river': 384,\n",
              " 'years': 385,\n",
              " 'almost': 386,\n",
              " 'own': 387,\n",
              " 'three': 388,\n",
              " 'half': 389,\n",
              " 'trip': 390,\n",
              " 'salad': 391,\n",
              " 'whole': 392,\n",
              " 'less': 393,\n",
              " \"couldn't\": 394,\n",
              " 'such': 395,\n",
              " 'checked': 396,\n",
              " 'fun': 397,\n",
              " 'inside': 398,\n",
              " 'may': 399,\n",
              " 'store': 400,\n",
              " \"isn't\": 401,\n",
              " 'sauce': 402,\n",
              " 'plus': 403,\n",
              " 'lake': 404,\n",
              " 'expensive': 405,\n",
              " 'charge': 406,\n",
              " '\\xa0this': 407,\n",
              " 'lounge': 408,\n",
              " 'usually': 409,\n",
              " 'elevators': 410,\n",
              " 'rate': 411,\n",
              " 'extremely': 412,\n",
              " \"doesn't\": 413,\n",
              " 'mile': 414,\n",
              " '\\xa0but': 415,\n",
              " '20': 416,\n",
              " \"i'll\": 417,\n",
              " 'favorite': 418,\n",
              " '6': 419,\n",
              " 'oh': 420,\n",
              " 'concierge': 421,\n",
              " 'second': 422,\n",
              " 'either': 423,\n",
              " \"wouldn't\": 424,\n",
              " 'valet': 425,\n",
              " 'him': 426,\n",
              " 'music': 427,\n",
              " 'pizza': 428,\n",
              " 'king': 429,\n",
              " 'tried': 430,\n",
              " \"\\xa0it's\": 431,\n",
              " 'easy': 432,\n",
              " 'decided': 433,\n",
              " 'plenty': 434,\n",
              " '15': 435,\n",
              " 'lots': 436,\n",
              " '\\xa0my': 437,\n",
              " 'sweet': 438,\n",
              " 'distance': 439,\n",
              " 'expect': 440,\n",
              " 'views': 441,\n",
              " 'later': 442,\n",
              " 'coming': 443,\n",
              " 'within': 444,\n",
              " 'hear': 445,\n",
              " 'red': 446,\n",
              " 'customer': 447,\n",
              " 'early': 448,\n",
              " 'several': 449,\n",
              " 'arrived': 450,\n",
              " 'itself': 451,\n",
              " 'quality': 452,\n",
              " 'fact': 453,\n",
              " 'guess': 454,\n",
              " 'cheap': 455,\n",
              " 'week': 456,\n",
              " 'tasty': 457,\n",
              " 'instead': 458,\n",
              " 'year': 459,\n",
              " 'tea': 460,\n",
              " 'fantastic': 461,\n",
              " '\\xa0there': 462,\n",
              " 'corner': 463,\n",
              " 'near': 464,\n",
              " 'done': 465,\n",
              " 'available': 466,\n",
              " 'blocks': 467,\n",
              " 'course': 468,\n",
              " 'problem': 469,\n",
              " 'style': 470,\n",
              " 'wifi': 471,\n",
              " 'makes': 472,\n",
              " 'finally': 473,\n",
              " 'beer': 474,\n",
              " 'enjoy': 475,\n",
              " 'reviews': 476,\n",
              " 'help': 477,\n",
              " 'tell': 478,\n",
              " 'comfy': 479,\n",
              " 'spacious': 480,\n",
              " 'care': 481,\n",
              " 'gym': 482,\n",
              " 'yes': 483,\n",
              " 'tiny': 484,\n",
              " 'spot': 485,\n",
              " 'seems': 486,\n",
              " 'recently': 487,\n",
              " 'keep': 488,\n",
              " 'name': 489,\n",
              " 'block': 490,\n",
              " 'per': 491,\n",
              " 'stuff': 492,\n",
              " 'family': 493,\n",
              " 'ice': 494,\n",
              " 'ave': 495,\n",
              " 'quick': 496,\n",
              " 'stop': 497,\n",
              " 'event': 498,\n",
              " 'might': 499,\n",
              " 'set': 500,\n",
              " 'trying': 501,\n",
              " 'point': 502,\n",
              " '\\xa0if': 503,\n",
              " 'does': 504,\n",
              " 'sleep': 505,\n",
              " 'paid': 506,\n",
              " 'cold': 507,\n",
              " 'walked': 508,\n",
              " '\\xa0and': 509,\n",
              " 'working': 510,\n",
              " '7': 511,\n",
              " 'guy': 512,\n",
              " 'short': 513,\n",
              " 'offer': 514,\n",
              " 'mini': 515,\n",
              " 'yet': 516,\n",
              " 'bathrooms': 517,\n",
              " 'club': 518,\n",
              " 'liked': 519,\n",
              " 'manager': 520,\n",
              " 'rather': 521,\n",
              " 'live': 522,\n",
              " 'meat': 523,\n",
              " 'conference': 524,\n",
              " 'evening': 525,\n",
              " 'myself': 526,\n",
              " 'bring': 527,\n",
              " 'amenities': 528,\n",
              " 'given': 529,\n",
              " 'ready': 530,\n",
              " 'husband': 531,\n",
              " 'entire': 532,\n",
              " 'walls': 533,\n",
              " 'between': 534,\n",
              " 'waiting': 535,\n",
              " 'absolutely': 536,\n",
              " 'card': 537,\n",
              " 'mind': 538,\n",
              " 'leave': 539,\n",
              " 'real': 540,\n",
              " 'anyone': 541,\n",
              " 'brought': 542,\n",
              " 'loud': 543,\n",
              " 'wall': 544,\n",
              " 'looks': 545,\n",
              " 'middle': 546,\n",
              " 'noise': 547,\n",
              " 'phone': 548,\n",
              " 'dining': 549,\n",
              " 'busy': 550,\n",
              " 'sushi': 551,\n",
              " 'taste': 552,\n",
              " 'head': 553,\n",
              " 'hilton': 554,\n",
              " 'chocolate': 555,\n",
              " 'four': 556,\n",
              " 'bread': 557,\n",
              " 'must': 558,\n",
              " 'group': 559,\n",
              " 'white': 560,\n",
              " 'light': 561,\n",
              " \"won't\": 562,\n",
              " 'convenient': 563,\n",
              " 'fan': 564,\n",
              " 'warm': 565,\n",
              " 'saturday': 566,\n",
              " '50': 567,\n",
              " 'past': 568,\n",
              " 'return': 569,\n",
              " 'reason': 570,\n",
              " 'avenue': 571,\n",
              " \"you'll\": 572,\n",
              " 'reception': 573,\n",
              " 'slow': 574,\n",
              " 'fish': 575,\n",
              " 'items': 576,\n",
              " 'saw': 577,\n",
              " 'wrong': 578,\n",
              " 'main': 579,\n",
              " 'window': 580,\n",
              " 'served': 581,\n",
              " 'choice': 582,\n",
              " 'expected': 583,\n",
              " 'quiet': 584,\n",
              " 'options': 585,\n",
              " 'show': 586,\n",
              " 'access': 587,\n",
              " 'standard': 588,\n",
              " 'reservation': 589,\n",
              " 'seem': 590,\n",
              " '\\xa0you': 591,\n",
              " 'fast': 592,\n",
              " 'shop': 593,\n",
              " '8': 594,\n",
              " 'flat': 595,\n",
              " 'cream': 596,\n",
              " 'start': 597,\n",
              " 'fried': 598,\n",
              " 'sandwich': 599,\n",
              " 'needs': 600,\n",
              " \"they're\": 601,\n",
              " 'heard': 602,\n",
              " 'run': 603,\n",
              " 'job': 604,\n",
              " 'complimentary': 605,\n",
              " 'level': 606,\n",
              " 'ago': 607,\n",
              " 'kept': 608,\n",
              " 'checking': 609,\n",
              " 'north': 610,\n",
              " 'glass': 611,\n",
              " 'spa': 612,\n",
              " 'pillows': 613,\n",
              " 'taking': 614,\n",
              " 'beef': 615,\n",
              " 'flavor': 616,\n",
              " 'sit': 617,\n",
              " 'downstairs': 618,\n",
              " 'double': 619,\n",
              " '\\xa0a': 620,\n",
              " 'book': 621,\n",
              " 'cab': 622,\n",
              " 'impressed': 623,\n",
              " 'under': 624,\n",
              " 'burger': 625,\n",
              " 'ended': 626,\n",
              " 'minute': 627,\n",
              " 'seen': 628,\n",
              " 'others': 629,\n",
              " 'rice': 630,\n",
              " 'crowd': 631,\n",
              " 'thin': 632,\n",
              " 'started': 633,\n",
              " 'reasonable': 634,\n",
              " 'type': 635,\n",
              " 'change': 636,\n",
              " 'windows': 637,\n",
              " 'mean': 638,\n",
              " 'offered': 639,\n",
              " 'magnificent': 640,\n",
              " 'yelp': 641,\n",
              " 'air': 642,\n",
              " 'lovely': 643,\n",
              " 'inn': 644,\n",
              " 'fridge': 645,\n",
              " 'spend': 646,\n",
              " 'okay': 647,\n",
              " 'already': 648,\n",
              " 'wish': 649,\n",
              " 'totally': 650,\n",
              " 'cooked': 651,\n",
              " 'products': 652,\n",
              " 'making': 653,\n",
              " 'guest': 654,\n",
              " 'worked': 655,\n",
              " 'bucks': 656,\n",
              " 'pleasant': 657,\n",
              " 'priceline': 658,\n",
              " 'disappointed': 659,\n",
              " 'atmosphere': 660,\n",
              " 'including': 661,\n",
              " 'sized': 662,\n",
              " 'cute': 663,\n",
              " 'dish': 664,\n",
              " 'completely': 665,\n",
              " 'towels': 666,\n",
              " 'non': 667,\n",
              " 'watch': 668,\n",
              " 'along': 669,\n",
              " '\\xa0so': 670,\n",
              " 'dessert': 671,\n",
              " 'paying': 672,\n",
              " 'boutique': 673,\n",
              " 'comes': 674,\n",
              " 'regular': 675,\n",
              " 'local': 676,\n",
              " 'upon': 677,\n",
              " 'spent': 678,\n",
              " 'bars': 679,\n",
              " 'dark': 680,\n",
              " 'turn': 681,\n",
              " 'loop': 682,\n",
              " 'screen': 683,\n",
              " 'cost': 684,\n",
              " 'tables': 685,\n",
              " 'bottle': 686,\n",
              " 'knew': 687,\n",
              " 'soup': 688,\n",
              " 'steak': 689,\n",
              " 'priced': 690,\n",
              " 'professional': 691,\n",
              " 'sitting': 692,\n",
              " 'highly': 693,\n",
              " 'quickly': 694,\n",
              " 'average': 695,\n",
              " 'cake': 696,\n",
              " 'surprised': 697,\n",
              " 'tower': 698,\n",
              " '\\xa0not': 699,\n",
              " 'starbucks': 700,\n",
              " 'due': 701,\n",
              " 'number': 702,\n",
              " 'birthday': 703,\n",
              " 'believe': 704,\n",
              " 'train': 705,\n",
              " 'gorgeous': 706,\n",
              " 'suites': 707,\n",
              " 'doing': 708,\n",
              " 'touch': 709,\n",
              " 'issue': 710,\n",
              " 'pier': 711,\n",
              " 'fries': 712,\n",
              " 'life': 713,\n",
              " 'bedroom': 714,\n",
              " 'hyatt': 715,\n",
              " 'guys': 716,\n",
              " 'rock': 717,\n",
              " 'nearby': 718,\n",
              " \"weren't\": 719,\n",
              " 'attentive': 720,\n",
              " 'note': 721,\n",
              " '00': 722,\n",
              " 'world': 723,\n",
              " 'fairly': 724,\n",
              " 'living': 725,\n",
              " 'sunday': 726,\n",
              " 'rude': 727,\n",
              " 'doors': 728,\n",
              " 'drake': 729,\n",
              " 'office': 730,\n",
              " 'pricey': 731,\n",
              " 'bill': 732,\n",
              " 'floors': 733,\n",
              " 'tub': 734,\n",
              " 'gets': 735,\n",
              " 'toilet': 736,\n",
              " 'above': 737,\n",
              " 'kids': 738,\n",
              " '12': 739,\n",
              " 'soft': 740,\n",
              " 'list': 741,\n",
              " 'roll': 742,\n",
              " 'hair': 743,\n",
              " 'pick': 744,\n",
              " '24': 745,\n",
              " 'rest': 746,\n",
              " 'behind': 747,\n",
              " 'navy': 748,\n",
              " 'read': 749,\n",
              " 'five': 750,\n",
              " 'eating': 751,\n",
              " 'wife': 752,\n",
              " 'nicely': 753,\n",
              " 'perfectly': 754,\n",
              " 'amount': 755,\n",
              " '9': 756,\n",
              " 'sign': 757,\n",
              " 'twice': 758,\n",
              " 'friday': 759,\n",
              " 'closet': 760,\n",
              " 'drive': 761,\n",
              " 'dishes': 762,\n",
              " 'class': 763,\n",
              " 'girl': 764,\n",
              " 'spicy': 765,\n",
              " 'man': 766,\n",
              " 'included': 767,\n",
              " 'bath': 768,\n",
              " 'green': 769,\n",
              " 'afternoon': 770,\n",
              " 'idea': 771,\n",
              " '11': 772,\n",
              " 'seriously': 773,\n",
              " 'key': 774,\n",
              " 'south': 775,\n",
              " 'complaint': 776,\n",
              " '\\xa0when': 777,\n",
              " 'art': 778,\n",
              " 'easily': 779,\n",
              " 'buffet': 780,\n",
              " 'market': 781,\n",
              " 'often': 782,\n",
              " 'hit': 783,\n",
              " '\\xa0in': 784,\n",
              " 'single': 785,\n",
              " 'thanks': 786,\n",
              " 'bags': 787,\n",
              " 'crowded': 788,\n",
              " 'heart': 789,\n",
              " 'neighborhood': 790,\n",
              " 'public': 791,\n",
              " 'charged': 792,\n",
              " 'server': 793,\n",
              " 'turned': 794,\n",
              " 'black': 795,\n",
              " 'packed': 796,\n",
              " 'game': 797,\n",
              " 'meeting': 798,\n",
              " 'soon': 799,\n",
              " 'summer': 800,\n",
              " 'kitchen': 801,\n",
              " 'waitress': 802,\n",
              " 'taken': 803,\n",
              " 'damn': 804,\n",
              " 'travel': 805,\n",
              " 'opened': 806,\n",
              " 'wi': 807,\n",
              " 'st': 808,\n",
              " 'sort': 809,\n",
              " 'brunch': 810,\n",
              " 'housekeeping': 811,\n",
              " 'counter': 812,\n",
              " 'giving': 813,\n",
              " 'older': 814,\n",
              " 'option': 815,\n",
              " 'value': 816,\n",
              " 'eggs': 817,\n",
              " 'ones': 818,\n",
              " 'fi': 819,\n",
              " 'reservations': 820,\n",
              " 'decorated': 821,\n",
              " 'station': 822,\n",
              " 'mention': 823,\n",
              " 'buy': 824,\n",
              " 'low': 825,\n",
              " \"haven't\": 826,\n",
              " 'remember': 827,\n",
              " 'sometimes': 828,\n",
              " \"\\xa0i'm\": 829,\n",
              " 'worst': 830,\n",
              " 'beat': 831,\n",
              " 'palmer': 832,\n",
              " 'company': 833,\n",
              " 'using': 834,\n",
              " '\\xa0however': 835,\n",
              " 'tip': 836,\n",
              " 'weird': 837,\n",
              " 'yourself': 838,\n",
              " 'westin': 839,\n",
              " 'visiting': 840,\n",
              " 'please': 841,\n",
              " 'mexican': 842,\n",
              " 'sink': 843,\n",
              " 'tasted': 844,\n",
              " 'security': 845,\n",
              " 'case': 846,\n",
              " 'areas': 847,\n",
              " 'anyway': 848,\n",
              " 'received': 849,\n",
              " 'online': 850,\n",
              " 'smaller': 851,\n",
              " 'closed': 852,\n",
              " 'upgraded': 853,\n",
              " 'ceiling': 854,\n",
              " 'everywhere': 855,\n",
              " 'poor': 856,\n",
              " 'interesting': 857,\n",
              " 'understand': 858,\n",
              " '\\xa0for': 859,\n",
              " 'waited': 860,\n",
              " 'noticed': 861,\n",
              " 'fitness': 862,\n",
              " 'c': 863,\n",
              " 'holiday': 864,\n",
              " 'website': 865,\n",
              " 'school': 866,\n",
              " 'chain': 867,\n",
              " \"aren't\": 868,\n",
              " 'request': 869,\n",
              " 'upgrade': 870,\n",
              " 'cozy': 871,\n",
              " 'hand': 872,\n",
              " 'certainly': 873,\n",
              " 'considering': 874,\n",
              " 'dry': 875,\n",
              " 'issues': 876,\n",
              " 'french': 877,\n",
              " 'james': 878,\n",
              " '25': 879,\n",
              " 'grab': 880,\n",
              " 'plate': 881,\n",
              " 'anywhere': 882,\n",
              " '\\xa0no': 883,\n",
              " 'seating': 884,\n",
              " 'based': 885,\n",
              " 'girls': 886,\n",
              " 'date': 887,\n",
              " 'unless': 888,\n",
              " 'feeling': 889,\n",
              " 'lady': 890,\n",
              " 'moved': 891,\n",
              " 'terrible': 892,\n",
              " 'stopped': 893,\n",
              " 'furniture': 894,\n",
              " 'plan': 895,\n",
              " 'http': 896,\n",
              " 'state': 897,\n",
              " 'accommodating': 898,\n",
              " 'goes': 899,\n",
              " 'sat': 900,\n",
              " 'fiancee': 901,\n",
              " 'fabulous': 902,\n",
              " 'somewhere': 903,\n",
              " '\\xa0our': 904,\n",
              " 'separate': 905,\n",
              " 'major': 906,\n",
              " 'shrimp': 907,\n",
              " 'smoking': 908,\n",
              " 'waiter': 909,\n",
              " 'months': 910,\n",
              " 'paper': 911,\n",
              " 'ride': 912,\n",
              " 'pork': 913,\n",
              " 'credit': 914,\n",
              " 'weeks': 915,\n",
              " 'hold': 916,\n",
              " 'ate': 917,\n",
              " 'chance': 918,\n",
              " 'talking': 919,\n",
              " 'la': 920,\n",
              " 'mentioned': 921,\n",
              " 'unfortunately': 922,\n",
              " 'incredibly': 923,\n",
              " 'm': 924,\n",
              " 'glad': 925,\n",
              " 'yummy': 926,\n",
              " 'overpriced': 927,\n",
              " 'grand': 928,\n",
              " 'points': 929,\n",
              " 'filled': 930,\n",
              " 'despite': 931,\n",
              " 'cut': 932,\n",
              " 'variety': 933,\n",
              " 'throughout': 934,\n",
              " 'property': 935,\n",
              " '40': 936,\n",
              " 'complaints': 937,\n",
              " 'pleased': 938,\n",
              " 'move': 939,\n",
              " 'except': 940,\n",
              " 'wireless': 941,\n",
              " 'says': 942,\n",
              " 'crazy': 943,\n",
              " '\\xa0also': 944,\n",
              " 'woman': 945,\n",
              " 'provided': 946,\n",
              " 'massage': 947,\n",
              " 'dirty': 948,\n",
              " 'otherwise': 949,\n",
              " 'luggage': 950,\n",
              " 'serve': 951,\n",
              " 'apparently': 952,\n",
              " 'sorry': 953,\n",
              " 'stand': 954,\n",
              " '\\xa0she': 955,\n",
              " 'thank': 956,\n",
              " 'historic': 957,\n",
              " '45': 958,\n",
              " 'boyfriend': 959,\n",
              " 'shops': 960,\n",
              " 'sheets': 961,\n",
              " '\\xa0he': 962,\n",
              " 'supposed': 963,\n",
              " 'save': 964,\n",
              " 'alone': 965,\n",
              " 'slightly': 966,\n",
              " 'typical': 967,\n",
              " 'fruit': 968,\n",
              " 'smell': 969,\n",
              " 'chairs': 970,\n",
              " 'pressure': 971,\n",
              " 'horrible': 972,\n",
              " 'cheaper': 973,\n",
              " 'whatever': 974,\n",
              " 'problems': 975,\n",
              " 'complain': 976,\n",
              " 'blue': 977,\n",
              " 'means': 978,\n",
              " 'simply': 979,\n",
              " 'immediately': 980,\n",
              " 'cleaning': 981,\n",
              " 'owner': 982,\n",
              " 'dog': 983,\n",
              " 'dollars': 984,\n",
              " 'choices': 985,\n",
              " 'literally': 986,\n",
              " 'lack': 987,\n",
              " 'b': 988,\n",
              " 'truly': 989,\n",
              " 'possible': 990,\n",
              " 'empty': 991,\n",
              " 'hotwire': 992,\n",
              " 'hope': 993,\n",
              " 'add': 994,\n",
              " 'total': 995,\n",
              " 'ambiance': 996,\n",
              " 'watching': 997,\n",
              " 'basically': 998,\n",
              " 'simple': 999,\n",
              " 'today': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "VOzK2GoK2hsu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then use the tokenizer to convert all texts in the training-set to lists of these tokens."
      ]
    },
    {
      "metadata": {
        "id": "Z4dSCl0F2YmJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_tokens = tokenizer.texts_to_sequences(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "um_Tps-A2t2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b9ffa500-ff34-4799-8ac5-2a1e856a7d37"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train[1]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The only place inside the Loop that you can stay for $55/night. Also, the only place you can have a picnic dinner and get a little frisky on the 17th floor roof and then wake up in your room the next morning to an army of ants going in on your picnic leftovers.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "-GKziOCE2zwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "040a70f4-4f0a-4a9b-9488-bccbae159ab9"
      },
      "cell_type": "code",
      "source": [
        "np.array(X_train_tokens[1])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   5,  773,   27, 1104, 1048,    7,   18,  408,    5,   27,   85,\n",
              "       2071,    1, 3823,    9,    3,  301,    7,  385,    2,    5,    6,\n",
              "        245,   33, 1348,   53,    5,  473,   27,    1, 2241,    4,   82,\n",
              "         30,   80,    1,  869,    7,   54,   37,    7,  299,  337,    5,\n",
              "         23,    4,  184,    5,    6,   95,  659,  203,  641,  326,    6,\n",
              "        113,   16,   53,   24,  178,   12,   24,  146,    4,   43,    1,\n",
              "         40,    5,  102,   98,  285,   30,    6,   64,  271,  296,   53,\n",
              "         14,  337,    2,    5,  450,    2,  931,  165,  263,   19,  348,\n",
              "         11,    6,   29, 1049,    4,   23,    4,  954,    8,    1,  507,\n",
              "        162,    1,   22,    1,  182, 5096,   22,   81,    9,    3,  301,\n",
              "          7,  201,    4,  153,   11,  590,  367,   73,   89,   11,  245,\n",
              "         10,   46, 3371,    4,   31,    1, 1162,    6, 1112,    2,    1,\n",
              "        251,   26,  145,  260,  346,   64,  446, 5320,    5, 1490,  149,\n",
              "          3,  337,   30,  130,  178,    1,   40,   10,  409, 5792,   80,\n",
              "       2223,  787,    2,    1, 1421,  130,  152,  108,   33, 1186,    5,\n",
              "         23,  147,  628,   29,  174, 3824, 1151,    2, 4577, 2337, 7732,\n",
              "          8,   14,  713,   30,    6,   72,   45,   33, 2156, 1743,   12,\n",
              "          7,  468,   39,    7,    1,   26, 9209,  124,    2, 4718,   14,\n",
              "        148,   29,    5,    6,   95, 5793,   39,    8,   39,   11,    6,\n",
              "          3,   66,    4,  478, 3164,   57,   13,  145,   25,    3,   66,\n",
              "          4,  153,  139, 2427,    4,  173,    3,  422,   63,  368,   97,\n",
              "         49,   25,   32,  281,   32,    5,  290])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "3rToCDDw3mUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "DhgXmCf73A4k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to convert the texts in the test-set to tokens."
      ]
    },
    {
      "metadata": {
        "id": "Vh1cpnNf290L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbUNpQnC3Nvl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Padding and Truncating Data¶\n",
        "The Recurrent Neural Network can take sequences of arbitrary length as input, but in order to use a whole batch of data, the sequences need to have the same length. There are two ways of achieving this: (A) Either we ensure that all sequences in the entire data-set have the same length, or (B) we write a custom data-generator that ensures the sequences have the same length within each batch.\n",
        "\n",
        "Solution (A) is simpler but if we use the length of the longest sequence in the data-set, then we are wasting a lot of memory. This is particularly important for larger data-sets.\n",
        "\n",
        "So in order to make a compromise, we will use a sequence-length that covers most sequences in the data-set, and we will then truncate longer sequences and pad shorter sequences.\n",
        "\n",
        "First we count the number of tokens in all the sequences in the data-set."
      ]
    },
    {
      "metadata": {
        "id": "VnonWXPc3VCC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens]\n",
        "num_tokens = np.array(num_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kE0eW9yh3nqT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The average number of tokens in a sequence is:"
      ]
    },
    {
      "metadata": {
        "id": "l_CvxbHQ33ZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82b6519d-c557-4bcf-8c3e-09a289af9ba1"
      },
      "cell_type": "code",
      "source": [
        "np.mean(num_tokens)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150.9283"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "rPRWwYok39Z-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The maximum number of tokens in a sequence is:"
      ]
    },
    {
      "metadata": {
        "id": "Mw1EM8DA3_-H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f31a1b4f-e7e5-48d8-9056-79f87f4dd05c"
      },
      "cell_type": "code",
      "source": [
        "np.max(num_tokens)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "949"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "-Fdl5ced4JZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The max number of tokens we will allow is set to the average plus 2 standard deviations."
      ]
    },
    {
      "metadata": {
        "id": "hww_AqTx4MEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbcc3a81-fb74-48e7-e287-96bc73d48dc3"
      },
      "cell_type": "code",
      "source": [
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "metadata": {
        "id": "5ta5aQ8f4Vc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66f61ba3-7a4b-4d8b-84eb-20fdc28928dd"
      },
      "cell_type": "code",
      "source": [
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9527"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "-gpSwjbO4biG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When padding or truncating the sequences that have a different length, we need to determine if we want to do this padding or truncating 'pre' or 'post'. If a sequence is truncated, it means that a part of the sequence is simply thrown away. If a sequence is padded, it means that zeros are added to the sequence.\n",
        "\n",
        "So the choice of 'pre' or 'post' can be important because it determines whether we throw away the first or last part of a sequence when truncating, and it determines whether we add zeros to the beginning or end of the sequence when padding. This may confuse the Recurrent Neural Network."
      ]
    },
    {
      "metadata": {
        "id": "f6Hsbc8W4hxw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pad = 'pre'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OCJdSfKF4kHQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens,\n",
        "                            padding=pad, truncating=pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ky_gNfeH5tus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens,\n",
        "                           padding=pad, truncating=pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7woOO57951Kk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"
      ]
    },
    {
      "metadata": {
        "id": "Nnxp4Gvd56L9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c795574b-afb7-4e01-ec6b-7a8124e5b3a5"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_pad.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, 406)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "t73XrNAg6B29",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The matrix for the test-set has the same shape:"
      ]
    },
    {
      "metadata": {
        "id": "QVS6Q1HW6DCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c30f6587-3b58-456b-92ca-59c9f92624d4"
      },
      "cell_type": "code",
      "source": [
        "X_test_pad.shape"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 406)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "ff2kzZjx6L1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change of sequence of tokens into padded sequence.Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1TuD19Tx6XM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "3619af7d-188f-4389-cdd3-a57f2dea6c8a"
      },
      "cell_type": "code",
      "source": [
        "np.array(X_train_tokens[1])\n",
        "X_train_pad[1]\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    5,  773,   27, 1104, 1048,    7,   18,  408,\n",
              "          5,   27,   85, 2071,    1, 3823,    9,    3,  301,    7,  385,\n",
              "          2,    5,    6,  245,   33, 1348,   53,    5,  473,   27,    1,\n",
              "       2241,    4,   82,   30,   80,    1,  869,    7,   54,   37,    7,\n",
              "        299,  337,    5,   23,    4,  184,    5,    6,   95,  659,  203,\n",
              "        641,  326,    6,  113,   16,   53,   24,  178,   12,   24,  146,\n",
              "          4,   43,    1,   40,    5,  102,   98,  285,   30,    6,   64,\n",
              "        271,  296,   53,   14,  337,    2,    5,  450,    2,  931,  165,\n",
              "        263,   19,  348,   11,    6,   29, 1049,    4,   23,    4,  954,\n",
              "          8,    1,  507,  162,    1,   22,    1,  182, 5096,   22,   81,\n",
              "          9,    3,  301,    7,  201,    4,  153,   11,  590,  367,   73,\n",
              "         89,   11,  245,   10,   46, 3371,    4,   31,    1, 1162,    6,\n",
              "       1112,    2,    1,  251,   26,  145,  260,  346,   64,  446, 5320,\n",
              "          5, 1490,  149,    3,  337,   30,  130,  178,    1,   40,   10,\n",
              "        409, 5792,   80, 2223,  787,    2,    1, 1421,  130,  152,  108,\n",
              "         33, 1186,    5,   23,  147,  628,   29,  174, 3824, 1151,    2,\n",
              "       4577, 2337, 7732,    8,   14,  713,   30,    6,   72,   45,   33,\n",
              "       2156, 1743,   12,    7,  468,   39,    7,    1,   26, 9209,  124,\n",
              "          2, 4718,   14,  148,   29,    5,    6,   95, 5793,   39,    8,\n",
              "         39,   11,    6,    3,   66,    4,  478, 3164,   57,   13,  145,\n",
              "         25,    3,   66,    4,  153,  139, 2427,    4,  173,    3,  422,\n",
              "         63,  368,   97,   49,   25,   32,  281,   32,    5,  290],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "cjoL0cMU6hzx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tokenizer Inverse Map**\n",
        "\n",
        "For some strange reason, the Keras implementation of a tokenizer does not seem to have the inverse mapping from integer-tokens back to words, which is needed to reconstruct text-strings from lists of tokens. So we make that mapping here."
      ]
    },
    {
      "metadata": {
        "id": "pE2-gWMz6nap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTEF0gl17EID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Helper-function for converting a list of tokens back to a string of words."
      ]
    },
    {
      "metadata": {
        "id": "249upoac7G93",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "    \n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpGZa1Bo7JMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4011ac53-8912-4d08-c42f-e958abc5a74b"
      },
      "cell_type": "code",
      "source": [
        "#For example, this is the original text from the data-set:\n",
        "\n",
        "X_train[1]\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The only place inside the Loop that you can stay for $55/night. Also, the only place you can have a picnic dinner and get a little frisky on the 17th floor roof and then wake up in your room the next morning to an army of ants going in on your picnic leftovers.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "metadata": {
        "id": "CX83b1hj7UM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "239d74c2-acd6-4814-c502-e427be82b868"
      },
      "cell_type": "code",
      "source": [
        "tokens_to_string(X_train_tokens[12])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"this is my review for the spa at dana i came here recently for a massage with i'm pretty sure that was her name and a facial with can't remember \\xa0 the massage was amazing \\xa0i had an 80 min swedish massage but she used all and really it towards my needs \\xa0she was very friendly and talkative when i met her but knew when to stop talking when the massage began thank god nothing worse than a masseuse talking too much \\xa0i have a really bad back and she really used deep pressure on those areas and wasnt afraid to get in there and work it lol the facial was good too i got a 50 min facial with it was my first time so the lady really educated me on everything she was talking a whole lot at first but i guess she kinda had to so i knew what was going on but then she let me relax so that was nice \\xa0she gave a small massage on my neck and head it was really bad and pointless something she should work on no pressure at all the amenities sucked a tiny little steam room in the locker room seats 2 people and 2 showers the relaxation lounge was a tiny room for 4 5 people to sit no snacks but the kiwi strawberry water was good all in all if u really are in need of a good massage and dont care for amenities then u should come see but if u want amenities and basically stuff that you're paying for go to a diff med spa\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "Q1lwXbDO7nTq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create the Recurrent Neural Network**\n"
      ]
    },
    {
      "metadata": {
        "id": "aUlcYzSR7war",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jOUcjmQA70D0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 8\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zEiHGsop7243",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "12035094-f827-4a19-f192-2de71cf3cd3f"
      },
      "cell_type": "code",
      "source": [
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens,\n",
        "                    name='layer_embedding'))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RSxS0bQ4762I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adding the gated GRU\n",
        "model.add(GRU(units=16, return_sequences=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3HEo-f3j8DP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adding the second GRU with 8 output units\n",
        "model.add(GRU(units=8, return_sequences=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuYNAtaR8Lel",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This code adds the third and final GRU with 4 output units. This will be followed by a dense-layer, so it should only give the final output of the GRU and not a whole sequence of outputs.\n",
        "\n",
        "model.add(GRU(units=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43DIqzSy8YrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "dd a fully-connected / dense layer which computes a value between 0.0 and 1.0 that will be used as the classification output."
      ]
    },
    {
      "metadata": {
        "id": "Iuy0Ns1Z8elX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(Dense(1#, activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5tMNpwiZ8jQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adam optimizer with the given learning-rate.\n",
        "optimizer = Adam(lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xd22QhtM8pir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compile the Keras model so it is ready for training.\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKp48z8I8t-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "7426f0a2-e5c2-4f3d-e607-5577eb7afc7f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.summary()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_embedding (Embedding)  (None, 406, 8)            80000     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 406, 16)           1200      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 406, 16)           1584      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 406, 8)            600       \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 4)                 156       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 83,545\n",
            "Trainable params: 83,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPleqvXQ8zJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train the Recurrent Neural Network**\n",
        "\n",
        "We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."
      ]
    },
    {
      "metadata": {
        "id": "KvcbfmwK87I_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1170
        },
        "outputId": "d56d9139-1d23-43a5-8b48-c8130a9b8794"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(X_train_pad, y_train,\n",
        "          validation_split=0.05, epochs=3, batch_size=64)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6650 samples, validate on 350 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-609e3fd626c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.fit(X_train_pad, y_train,\\n          validation_split=0.05, epochs=3, batch_size=64)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3059\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3060\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3061\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'NR'"
          ]
        }
      ]
    }
  ]
}